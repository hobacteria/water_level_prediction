# -*- coding: utf-8 -*-
"""한강_lgbm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IuPC44he-KdBPDRCrcn_SK6Xan4hxD4j

### 필요한 라이브러리 불러오기
"""

import pandas as pd
import numpy as np

from glob import glob
from tqdm import tqdm
from scipy import interpolate
import tensorflow as tf

import matplotlib.pyplot as plt

"""### 데이터 전처리"""

path = 'C:/competition_data'

w_list = sorted(glob(path + "/water_data/*.csv"))
w_list
r_list = sorted(glob(path + "/rf_data/*.csv"))
r_list
submission_data = pd.read_csv(path + '/submission_data.csv',index_col = ('ymdhm'))

def get_train_data(river, num_slice,num):
  train_data = []
  train_label = []  

  for i in range(1,10):
      tmp = pd.read_csv(w_list[i])
      tmp = pd.concat([tmp, pd.read_csv(r_list[i])], axis = 1)
      tmp = tmp.replace(" ", np.nan)
      tmp = tmp.interpolate(method = 'values')
      tmp = tmp.fillna(0)

      for j in tqdm(range(len(tmp)-144)):

        train_data.append(np.array(tmp.loc[j:j + 143, ["tototf", "tide_level",
                                                        "rf_10184100","rf_10184110",
                                                        "rf_10184140","wl_1018662","fw_1018662","wl_1018680", "wl_1018683","fw_1018683", "wl_1019630","fw_1019630"]]).reshape(-1,).astype(np.float16))
          
        train_label.append(tmp.loc[j + 144, river].astype(np.float16))
      
  train_data = np.array(train_data)
  train_data = train_data[::num_slice,:]
  train_label = np.array(train_label).reshape(-1,)
  train_label=train_label[::num_slice]

  return train_data, train_label

"""### 추론 데이터셋 만들기"""

## val, test 데이터셋 불러오기

def get_test_data(river,num):
  test_data = []
  test_label = []
  val_data = []
  val_label = []

  tmp = pd.read_csv(w_list[-1],index_col = 0)
  tmp = pd.concat([tmp, pd.read_csv(r_list[-1],index_col = 0)], axis = 1)
  tmp.loc[submission_data.index,("wl_1018662", "wl_1018680",  "wl_1018683", "wl_1019630")] = submission_data.loc[:,:]
  tmp = tmp.replace(" ", np.nan)
  # 이전값을 사용
  tmp = tmp.fillna(method = 'pad')
  tmp = tmp.fillna(0)
  tmp  = tmp.reset_index()
  #tmp.loc[:, ["wl_1018662", "wl_1018680", "wl_1018683", "wl_1019630"]] = tmp.loc[:, ["wl_1018662", "wl_1018680", "wl_1018683", "wl_1019630"]]*100
      
  for j in tqdm(range(len(tmp)-144)):
      
      if j < 4320:
          val_data.append(np.array(tmp.loc[j:j + 143, ["tototf", "tide_level",
                                                            "rf_10184100","rf_10184110",
                                                            "rf_10184140","wl_1018662","fw_1018662","wl_1018680", "wl_1018683","fw_1018683", "wl_1019630","fw_1019630"]]).reshape(-1,).astype(np.float16))
              
          val_label.append(tmp.loc[j + 144, river].astype(np.float16))
          
      else:
          test_data.append(np.array(tmp.loc[j:j + 143, ["tototf", "tide_level",
                                                            "rf_10184100","rf_10184110",
                                                            "rf_10184140","wl_1018662","fw_1018662","wl_1018680", "wl_1018683","fw_1018683", "wl_1019630","fw_1019630"]]).reshape(-1,).astype(np.float16))
              
          test_label.append(tmp.loc[j + 144, river].astype(np.float16))

  test_data = np.array(test_data)
  test_label = np.array(test_label).reshape(-1,)
  val_data = np.array(val_data)
  val_label = np.array(val_label).reshape(-1,)
  return val_data, val_label, test_data, test_label

"""### 모델링 및 모델 학습

모델 학습, 예측값 반환과 예상 점수수
"""

import sklearn.metrics
from sklearn.metrics import mean_squared_error
import lightgbm as lgb
import joblib
import copy
from sklearn.model_selection import train_test_split
import eli5
from eli5.sklearn import PermutationImportance



def train(i):
  river_name = ["wl_1018662","wl_1018680", "wl_1018683", "wl_1019630"]

  train_data, train_label = get_train_data(river_name[i],num_slice = 1, num = i)
  val_data, val_label, test_data, test_label = get_test_data(river_name[i],num = i)

  lgb_train = lgb.Dataset(train_data, train_label)
  lgb_val = lgb.Dataset(val_data, val_label)

  params = {
      'boosting_type': 'gbdt',
      #'device_type': 'gpu',
      'metric': 'rmse',
      'num_leaves' : 100,
      'num_iterations': 10000,
      'learning_rate': 0.1,
      'early_stopping_rounds' : 50,
      'max_depth': 10
      #'bagging_fraction': 0.7,
      #'feature_fraction': 0.9,
      #'bagging_freq' : 10,


  }
  print('Starting training...')

  # train
  gbm = lgb.train(params,
                  lgb_train,
                  valid_sets = lgb_val       
                  )
  print('Saving model...')

    
  perm = PermutationImportance(gbm, scoring = "f1", random_state = 4885).fit(test_data, test_label)
  eli5.show_weights(perm, top = 80,)
  # save model

  print('Starting predicting...')
  # predict
  pred = gbm.predict(test_data, num_iteration=gbm.best_iteration)
  # eval
  rmse = mean_squared_error(test_label, pred) ** 0.5
  r2 = sklearn.metrics.r2_score(pred,test_label)

  print('The rmse of prediction is:', rmse)
  print('score is:', rmse/r2)
  return pred
#%%
sample_submission = pd.read_csv(path + "/sample_submission.csv")

trial = '2022_08_15_4' # 3번째, 중요한 변수 찾기
for i in range(4):
  pred = train(i)
  sample_submission.iloc[:,i+1] = pd.DataFrame(pred)

sample_submission

"""### 제출 파일 만들기"""

mse = mean_squared_error(submission_data, sample_submission.iloc[:,1:])
r2 = sklearn.metrics.r2_score(submission_data, sample_submission.iloc[:,1:])

print('r2:', r2)
print('rmse:', mse ** 0.5)
print((mse**0.5)/r2)
#%%
sample_submission.to_csv(path + f"/{trial}.csv", index = False)
